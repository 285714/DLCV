\relax 
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Visualization of the weights vectors and example data images. The darker parts in the visualization of the weights vectors look similar to the real example images.}}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces (c). Visualization of training accuracies while removing convolutional layers. Removing each convolutional layers increases the number of parameters. If we remove the first layer, the convolutional layer will have 64 feature maps; if we remove the second layer, it still have 64 feature maps. But no matter which layer is removed, since a max-pooling operation is also missing, the feature maps' size become twice of that with two layers and two max-pooling operations. From the figure we can see that the training accuracies with only one convolutional layer are lower than that with both layers. The test accuracy with both convolutional layer is around 97\%, only with the 1st layer is around 95.1\% and only with the 2nd layer is around 95.2\%. Additionally, the training time only with the 2nd convolutional layer is apparently longer than only with the 1st convolutional layer.}}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces (d). Visualization of training accuracies with changing the step size. Too small step size will make the gradient descent progress so slowly that within the iterations it cannot reach or come out of a local minimum; Too large step size will make the gradient descent travel between two sides of a (local) minimum and never reach it.}}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Visualization of training MNIST with different strategies.}}{4}}
